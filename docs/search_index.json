[["index.html", "Data Science Project Guide: Netflix 1 Welcome!", " Data Science Project Guide: Netflix TechAcademy e.V. Wintersemester 2022/23 1 Welcome! Let us start with an overview of what is waiting for you in the next weeks and in this project guide! If you are a beginner,in the first few chapters you will be introduced to the basics of the R and Python tracks respectively and you will find helpful explanations to questions you might have in the beginning of your coding journey. There will be a quick introduction to the Data Science track so that you can get started with the project quickly. So lets get started with the basics! You will work on your project in small groups of fellow students. This not only helps you get the project done faster, it also helps make your results even better. Our experience shows: Contrasting university backgrounds and different opinions and ideas will produce the best results. Besides, it is of course more fun to work on a project together than to code alone! The groups can consist of a maximum of four members and you can choose your teammates independently. We explicitly encourage you to collaborate with students from different departments. When submitting your final project, it is important to note: for a certificate, each person must submit the project individually. However, this can be identical within your group. You can get more information at our Introduction to Coding Workshop on November 9, 2022 or at our first Coding Meetup on November 23, 2022. This Netflix case study and the associated project guide was developed and written entirely from scratch by TechAcademys Data Science team. Jessica Weigel and Isabel Schnorr developed the project in R, while Lea Karoza, Moritz Schwerdt and Rocky Auer developed it in Python. "],["whats-data-science-and-how-do-i-do-it.html", "2 Whats Data Science and How Do I Do It? 2.1 Whats R and Datacamp Courses? 2.2 Whats Python and Datacamp Courses? 2.3 Your Data Science Project", " 2 Whats Data Science and How Do I Do It? 2.1 Whats R and Datacamp Courses? R is a programming language that was developed by statisticians in the early 90s for use in the calculation and visualization of statistical applications. R is now one of the most widely used programming languages in the field of data science. Code in R does not have to be compiled, but can be used interactively and dynamically. This makes it possible to quickly gain basic knowledge about existing data and to display it graphically. R offers much more than just programming, but also a complete system for solving statistical problems. A large number of packages and interfaces are available, with which the functionality can be expanded and integration into other applications is made possible. 2.1.1 RStudio Install R and RStudio locally To get started you should install R and RStudio locally on your computer. Before we try to put in word how to do so: Heres a DataCamp tutorial on how to do that. If you struggle, we will help you with installing at the Introduction to Coding Workshop or at our first Coding Meetup. Until then, focus on learning the hard skills of programming with the courses on DataCamp. That brings us to your curriculum in the next section! 2.1.2 DataCamp courses and Curriculum The following list shows the required DataCamp courses for the Data Science with R Track at TechAcademy. As a beginner, please stick to the courses of the beginner program. Ambitious beginners can, of course, take the advanced courses afterward. However, it would be best if you worked through the courses in the order we listed them. The same applies to the advanced courses. Here, too, you should finish the specified courses in the given order. Since it can, of course, happen that you have already mastered the topics of an advanced course, you can replace some courses. If you are convinced that the course does not add value to you, feel free to replace it with one of the courses in the Exchange Pool (see list below). To receive the certificate, both beginners and advanced learners must complete at least 6 courses of the curriculum (6/7 courses). After completing the curriculum and the projects requirements, you will receive your TechAcademy certificate! Data Science in R Fundamentals (Beginner) Introduction to R (4h) Intermediate R (6h) Data Manipulation with dplyr (4h) Cleaning Data in R (4h) Introduction to Data Visualization with ggplot2 (4h) Exploratory Data Analysis in R (4h) Reporting with R Markdown (4h) Data Science in R (Advanced) Intermediate R (6h) Data Manipulation with dplyr (4h) Cleaning Data in R (4h) Introduction to Data Visualization with ggplot2 (4h) Text Mining with Bag-of-Words in R (4h) Writing Efficient R Code (4h) Reporting with R Markdown (4h) Data Science in R (Advanced)  Exchange Pool Intermediate Data Visualization with ggplot2 Interactive Maps with leaflet in R (4h) Interactive Data Visualization with plotly in R (4h) Multiple and Logistic Regression in R (4h) Machine Learning in Tidyverse (5h) Introduction to Writing Functions in R (4h) 2.1.3 Helpful Links RStudio Cheat Sheets RMarkdown Explanation (to document your analyses) StackOverflow (forum for all kinds of coding questions) CrossValidated (Statistics and Data Science forum) 2.2 Whats Python and Datacamp Courses? Python is a dynamic programming language. You can execute the code in the interpreter, so you do not have to compile the code first. This feature makes Python very easy and quick to use. The excellent usability, easy readability, and simple structuring were and still are core ideas in developing this programming language. You can use Python to program according to any paradigm, whereby structured and object-oriented programming is most straightforward due to the structure of the language. Still, functional or aspect-oriented programming is also possible. These options give users significant freedom to design projects the way they want and great space to write code that is difficult to understand and confusing. For this reason, programmers developed specific standards based on the so-called Python Enhancement Proposals (PEP) over the decades. 2.2.1 Google Colab, Anaconda and Jupyter Normally before you can use Python, it must be installed on the computer. In order to make it easier to use Python right out of the box, we use Google Colab as an online Notebook. Using Google Colab will be proficient enough for the TechAcademy DataScience Course. If you want to expand your view, you can circle back to this Introduction and decide Python and Jupyter on your machine to play around with. Python is already installed on Linux and Unix systems (such as macOS), but often it is an older version. Since there are differences in the handling of Python version 2  which is not longer supported anymore  and version 3, we decided to work with version 3.6 or higher. One of the easiest ways to get both Python and most of the best known programming libraries is to install Anaconda. There are detailed explanations for the installation on all operating systems on the website of the provider. With Anaconda installed, all you have to do is open the Anaconda Navigator and youre ready to go. There are two ways to get started: Spyder or Jupyter. Spyder is the integrated development environment (IDE) for Python and offers all possibilities from syntax highlighting to debugging (links to tutorials below). The other option is to use Jupyter or Jupyter notebooks. It is an internet technology based interface for executing commands. The big advantage of this is that you can quickly write short code pieces and try them out interactively without writing an entire executable program. Now you can get started! If you have not worked with Jupyter before, we recommend that you complete the course on DataCamp first. There you will get to know many tips and tricks that will make your workflow with Jupyter much easier. In order to make your work and, above all, the collaboration easier, we are working with the Google Colab platform that contains a Jupyter environment with the necessary libraries. You can then import all the data necessary for the project with Google Drive. We will introduce this environment during our first Coding Meetup. Until then, focus on learning the hard skills of programming with your courses on DataCamp. This brings us to your curriculum in the next section. 2.2.2 DataCamp courses and Curriculum The following list shows the required DataCamp courses for the Data Science with Python Track at TechAcademy. As a beginner, please stick to the courses of the beginner program. Ambitious beginners can, of course, take the advanced courses afterward. However, it would be best if you worked through the courses in the order we listed them. The same applies to the advanced courses. Here, too, you should finish the specified courses in the given order. Since it can, of course, happen that you have already mastered the topics of an advanced course, you can replace some courses. If you are convinced that the course does not add value to you, feel free to replace it with one of the courses in the Exchange Pool (see list below). To receive the certificate, both beginners and advanced learners must complete at least 6 courses of the curriculum (6/7 courses). After completing the curriculum and the projects requirements, you will receive your TechAcademy certificate! To receive the certificate, both beginners and advanced learners must complete at least 6 courses of the curriculum (6/7 courses). Data Science with Python Fundamentals (Beginner) Introduction to Data Science in Python (4h) Intermediate Python (4h) Data Manipulation with pandas (4h) Cleaning Data in Python (4h) Introduction to Data Visualization with Matplotlib (4h) Exploratory Data Analysis in Python (4h) Python for Data Science Toolbox (Part 1) (3h) Data Science with Python (Advanced) Intermediate Python (4h) Data Manipulation with pandas (4h) Cleaning Data in Python (4h) Introduction to Data Visualization with Matplotlib (4h) Python Data Science Toolbox (Part 2) (4h) Feature Engineering for NLP in Python (4h) Writing Efficient Code with pandas (4h) Data Science with Python (Advanced) - Exchange Pool Interactive Data Visualization with Bokeh (4h) Data Visualization with Seaborn (4h) Introduction to Data Visualization with Plotly in Python 4h) Intermediate Regression with Statsmodels in Python (4h) Machine Learning in Scikit-learn 4h) Writing Efficient Python Code (4h) 2.2.3 Helpful Links Official Tutorials/Documentation: https://docs.python.org/3/tutorial/index.html https://jupyter.org/documentation Further Explanations: https://pythonprogramming.net/ https://automatetheboringstuff.com/ https://www.reddit.com/r/learnpython https://www.datacamp.com/community/tutorials/tutorial-jupyter-notebook 2.3 Your Data Science Project 2.3.1 Prelude Now that you have learned the theoretical foundation of Data Science in the DataCamp courses, you can put your skills into practice. We have put together a project for you based on real data sets. You can read about the details of this project in the following chapters of this project guide. If you still feel a little unsure about how to start off, there will be a Coding Introduction on November 9, 2022, where we will give you a general sense on how to start off with Python or R. Of course, we will also describe the project. We will discuss everything you need to know during the first Coding Meetup, which will take place on November 23, 2022. After that, your work on the project will officially begin. You can find the exact project tasks together with further explanations and hints in the following chapters. 2.3.2 Coding Meetups To give you a little overview on the dates of our Meetups, you can find all the dates in one go here! 02.11.2022, Kick-Off Event 09.11.2022, Introduction to Coding Workshop 23.11.2022, Coding MeetUp 1 14.12.2022, Coding MeetUp 2 11.01.2023, Coding MeetUp 3 05.02.2023, Coding MeetUp 4 To receive the certificate, it is essential that you hand in your project at least 3 times during the semester. The following dates are due dates: 2.3.3 Final Deadline For both your DataCamp courses and the project files hand-in the deadline is 05.02.2023, 23:59 "],["introduction-to-your-project.html", "3 Introduction to Your Project 3.1 Purpose of the Project Guide 3.2 What is this Project About? 3.3 Exploratory Data Analysis  Getting to Know the Data Set 3.4 Content-Based Recommendation System", " 3 Introduction to Your Project 3.1 Purpose of the Project Guide This document will guide you through the different steps of your project and will provide you with valuable hints along the way. However, it is not a detailed step-by-step manual because we felt like you needed to develop the skills of coming up with your way of solving different tasks. This method is a great way to apply the knowledge and tools you have acquired through DataCamp. Since data science concepts are independent of specific programming languages, we will describe the general approach in a text chunk. Having understood the bigger picture and starting with the tasks, you will find language-specific tips and tricks in visually separated boxes (R -Track : blue-bordered boxes, Python - Track : yellow-bordered boxes). Questions might come up, or you might not know how to solve a task right awaybut dont worrythis is just part of coding. In those cases, you could ask your fellow team member for help. If they are not able to help, and in the unlikely case that even Google cant help you, the TechAcademy mentors will help you via Slack or directly during the coding meetups. At the end of the project guide, you will find an overview of all tasks that have to be completed, depending on your track (beginner/advanced). You can use this list to check which tasks you still need to complete or which assignments are relevant for your track. 3.2 What is this Project About? This semester, we will have a look at Netflix data! More precisely, we are first analyzing a very detailed general Netflix data set, compiled on kaggle, after which you will get the chance to look at your own Netflix data. You will find all kinds of information in the data sets - valuable and useless ones. Are you already curious to see for yourself? In analogy to the typical Data Science workflow, we split this project into two parts. 3.3 Exploratory Data Analysis  Getting to Know the Data Set This first part of the project is structured in a way that lets you get to know the data thoroughly by completing the given tasks one after the other. As a beginner, you can stop after this part because you will have fulfilled the necessary coding requirements for the certificate. However, if this first part inspires you to learn more, we encourage you to also work on the second part. If you get stuck, Google and StackOverflow are amazing problem solver (besides the mentors, of course). 3.4 Content-Based Recommendation System This part is mainly for the advanced TechAcademy participants. If you are a beginner and you were able to complete the first part without too many difficulties, we highly recommend trying to do the second part as well. "],["exploratory-data-analysis.html", "4 Exploratory Data Analysis 4.1 Getting started 4.2 Data Cleaning and Useful Transformations 4.3 Your personal data 4.4 Lets get personal 4.5 Binge watching 4.6 Scatterplot with marginal density 4.7 Word cloud with your favorite genre 4.8 Surprise Us!", " 4 Exploratory Data Analysis Before you can dive into the data, you need to do two things. First of all, you need to request your personal data from Netflix, which can take a few days. Therefore, you should ask for your data as early as possible. (If you do not have access to a Netflix account, contact your mentor, and we will provide you with some data) Download your data Follow this link to request your data, and sign in with your username and password. After that, click on the red box Submit Request. Now you need to verify your request via Mail. Once you confirm the request, you will be forwarded to the website and have to enter your password again. If your request is in progress you might have to wait a few days until the data is available to download. When you receive the mail Your download is ready, you can download your data by clicking on the red box Get Information. Dont wait too long since you only have 7 days to download it from your account! After confirming your password, you can download your data as a netflix-report.zip file. Unzip the file, open the folder CONTENT_INTERACTION and look for the ViewingActivity.csv file. However, you can already start with the first exercises of this chapter with data we provide. To start right away, you need to set up your programming environment. This will be the place where the magic happens. We work with R and RStudio locally which requires a setup. If you havent installed it yet here is a DataCamp tutorial. If you struggle, we will help you with installing at the Introduction to Coding Workshop or at our first Coding Meetup We recommend using Google Colab for this project since it requires no particular setup, stores its notebooks to your Google Drive, and makes it easy for you to share them with your team members. As an alternative to Google Colab, you might want to install Jupyter Notebook locally using the Anaconda distribution. We will give you a more detailed step-by-step demo during the Introduction to Coding Workshop or our first Coding Meetup. 4.1 Getting started 4.1.1 Discovering the Data The first big step is importing a general Netflix data set into your coding environment: Netflix Once done, lets start by looking at the Netflix data set. Have a look at the columns of the data set and their values Do you see any missing values or data entries that are different from the other entries? What are the data formats (e.g. data types)? Look at the date_added columns. Do you see any critical aspects of the data? Write down a couple of sentences to these questions, the goal is to show (your readers) what you are seeing. Also, comment on any errors or irregularities which you notice and that could be an issue later on in the project. You can import data files by, for examples, using the command netflix_general &lt;- read_csv(Netflix.csv). However, it has to be the correct path. Use head() and glimpse() or class()(on a specific colum) to get an overview. Lastly, get a quick summary of the data using {base} Rs summary() or {psych}s describe() function. To find more ways to generate quick data summaries in R, check this blog post from Adam Medcalf. If you need some additional, more general information on how to import data and different data types, check out this cheat sheet. You can feed the links to the respective data files above to a method of the pandas package (you might want to specify the index column). Check out the resulting pd.DataFrame instance with the head() method and the dtpyes attribute. You can also dig into a specific column with describe(). Heres an additional pandas cheat sheet for you to reference 4.1.2 Give some overall statements Now that you have imported the data set, lets have a deeper look. Since you already got a feeling by now, it would be interesting to indicate some outstanding features (based on the uncleaned and untransfomated dataset)! Whats the longest movie (not TV show) included in the dataset? (Look very closely if your result is valid) Which country released the most content (movies and tv shows)? How many movies and tv shows are included? Try to do a simple plot here so you can see the distribution. If you cannot plot it just now, come back later to finish this. You could use {dplyr}s select, filter, count and arrange function to compute the desired outcome to answer the question. If you havent heard of the {dplyr} package yet, take the respective DataCamp course asap! For the plot, look at {ggplot} and then call geom_bar(). {ggplot2}s syntax is perhaps a bit harder to learn at the start, but it gives you more plotting benefits in the long run. If you havent heard of the {ggplot2} package yet, take the respective DataCamp course. In order to manipulate big amounts of data, Pandas implements different functions to manipulate entries in the Dataframe. Useful functions are: select, filter, value_counts and max. If some of those functions seem new to you, dont scare away from them just now. When it comes to packages and their implementations, there are usually some easy to understand examples that can be found on their wiki page. Keep in mind that the country data is saved as a list, pandas offers the explode function that helps us with that. In order to display graphs, we recommend using seaborn as a package. Seaborn offers a wide variety of different graphs options, it is important that seaborn just translates your input into MatplotLib commands. So if you feel unsatisfied with the customisation options in seaborn, you can always build your own graph using MatplotLib. To display the amount of movies and tv-shows a seaborn.countplot can be used. 4.2 Data Cleaning and Useful Transformations 4.2.1 Date Formatting From exploring the data in the previous tasks you might have noticed that the time and dates are not in a date format. In order to fix this, convert the date_added and release_year column into date format. You can use the lubridate package to transform the column into a date format. To make things easier, check out the lubridate cheat sheet. Converting the release year might take two steps. pd.to_datetime() is what I would look at for example. A corresponding DataCamp resource is section 4 in Working with dates and times in Python. Also, the Data Manipulation with pandas course is of great help for the following exercises 4.2.2 More details of the longest movie So far, you have converted the column into a more appropriate format for further investigations. Can you also fix the duration of the movies into a numeric format? Can you tell now what the longest movie is? On the concept of distribution, you should also try to compute the mean and standard deviation of the movie duration in minutes. First off, you might want to create a separate data frame for the movies so the tv shows do not bother you further. Use the filter call here. Next you might have recognized the min appendix. This can be deleted with the gsub call. With the help of mutate you can convert the column into numeric values and arrange the movies descending. For the mean and standard deviation simply use {base} Rs mean() function to compute a mean of any column in R. Similarly, R comes with the sd() function to calculate the standard deviation of a column. You can combine these two functions, e.g., in a {dplyr}s summarise() verb. First off, you might want to create a separate data frame for the movies so the tv shows do not bother you further. Try to find a useful criteria in the data that separates series and movies in order to filter out just the movies. To get the numeric value out of the string either a separation by space or match by regex will work. Make sure to convert the string text to a more suitable number type such as integer. Pandas already helps when finding out the standard deviation and mean with the implemented functions std() and mean(). 4.2.3 A histogram of the top 10 longest movies duration Visualizing data is essential to facilitate perception and to understand information: Create a graph to visualize the top 10 longest movies. We focus on creating a histogram in the tips section since its perhaps the most common approach for plotting. You can, however, choose a different chart type; just make sure that the information you want to display is clear and correct. However, as in school, always add axis labels when possible. If your data is saved as a dataframe you can perceive with the data from the previous task. Use {ggplot2} to create a histogram. Integrate top_n() before the ggplot() call and select geom_col() to achieve a histogram presentation. How exactly you do this is up to you. An easy approach might be to sort the dataframe with movie durations in descending order and select only the first ten columns for a barplot. Look into pandas.nlargest() for more information. 4.2.4 Visualizing movie durations over time While we are at the topic of movie durations, it might also be interesting to see how the average movie length evolved: Choose and plot the chart you think is most appropriate for displaying this type of information. After plotting, please comment and interpret the graph: Were there any significant increases/decreases in movie length over time? If so, what could be the reason? Again, its ggplots turn. Instead of preparing the summarise() and mean() functions combination you can also directly integrate the adjustments in the geom_bar call with (stat = summary, fun = mean). In order to summarize the mean movie length, look into the functions groupby() to group all movies by year and then find the average length mean(). Make sure that every year in the dataframe is present. In order to plot two plots on one axis, declare the axis as one plot and use the function axis.twinx(). 4.3 Your personal data 4.3.1 Load your data You will now use the data set which you requested from Netflix. In the Netflix folder you will find the document of interest: ViewingActivity. Load this in your environment and inspect it as you did before with the Netflix Dataset. If you cant request your data, ask your mentor; they will provide you with an alternative data set. 4.3.2 Clean and transform dataset As you might have noticed, Netflix recorded every time you clicked on a movie even if you didnt watch it. Check which column indicates those with a specific value. To avoid a bias in the following analysis, delete the respective rows. You can further drop columns that seem unnecessary or dont give any information and change column names if you wish. Additionally, convert the column Duration in minutes (round them) and extract the viewing day and viewing time from Start time into two separate columns (e.g., date and time). mutate and filter are your best friends in this cleaning process. The most useful course for this task is probably Cleaning Data in Python. Use datetime functions, drop columns, filter by time. 4.3.3 Merging datasets: primitive approach As a data scientist, youll often find yourself working with data sets from different data sources referencing the same object. For example, you might have the movie names in one file and the respective genre in a separate file. It would make more sense to just merge the two data sets into one. Indeed, this is the case with our data. Your Netflix data does not provide information on genre, actor, or director, while the general Netflix data set does. So, to make life easier for the upcoming tasks, youll now need to merge both data sets by the title name. There are several things kind of wrong with the merged data set. What is it and why? Tipp: You need to prepare the dataset in a way that the title, session and episode will be split into a column of title, session and episode separately in order for the two data frames to join each other properly. Calls such as separate(), ifelse() and rename() are suitable here. Merging can be done in several ways. You could, e.g., use {dplyr}s left_join() to combine the two data sets based on the left sided dataset. Which one should be the left-sided? The merge function allows for different types of merges. Try coming up with the most logical one in this scenario! 4.3.4 Merging datasets: advanced approach You might have noticed that merging the data set is a little challenging. This is because Netflix sometimes displays title, session and episode in a remixed way and also because your Netflix data might be in German whereas the general Netflix Data is in English. We fixed this issue externally and you can now send your data to Moritz Schwerdt, who will provide you with a respective general Netflix dataset. Now try merging again! 4.3.5 Dynamic line plot Your goal for this task is to plot how each viewers activity was recorded over time. Since it would be a bit unclear in a static plot, lets try to do a dynamic chart here! The data is already ready for this. Your ggplot() call might need some more input than you used before. As the graphic is supposed to be a line with points geom_line() and geom_point () will be a good choice of combination. The dynamic aspect is no magic, you just need to add the transition_reveal() call. 4.4 Lets get personal We dived into the provided gerneral Netflix data set and a bit into your overall personal Netflix dataset, but now is the time to look into your very own Netflix history. First, we want to look at the longest movie you have ever watched, afterward, into your general and binge-watching behavior. We recommend using Tobis data if you are currently working with the Netflix data set provided from us. So, whats the longest movie you have ever watched? Maybe you also want to check the day? Do you remember that day? filter, select and arrange will do a good job here. Useful functions might be sort_values() and groupby(). 4.4.1 Monthly viewing time in 2021 Lets get a little fancy by displaying the monthly average time you watched in 2021 (or another year of your choice). This task involves three steps. First, you want to extract the month from the start_time column, where lubridate will help you. You can then use the group_by argument to group the dataset by the month and filter for a specific year. After that, you can pipe along and directly add the third step: your ggplot call. Now, you can visualize your output however you want. The presented output is obtained by combining geom_col + coord_polar(). We recommend a stepwise course of action: first, extract the month from your date column - at this point you should have a clue which package might be able to help! Then, you should groupby month and filter for a year of your choice. The last step is the generation of your plot! A great package for this task is plotly.graph_objects. You might want to play around with the Barpolar function a little to receive the desired output. 4.4.2 Average per weekday Our subsequent interest lies in analyzing the viewing time of specific weekdays. Either choose the whole year or a month of your interest and look at the weekday average. On which days have you watched more Netflix, can you see a peak? For our advanced programmers: Would it not be interesting to look at what time of the day you watch the most? As before, you now want to extract the weekday from the start_time. Further you might want to filter for a specific month or year and group by date and weekday. You will then need to sum the minutes per day and group again by weekday to summarize for the mean of the duration. Finally, you could, for example, plot a classic ggplot. The procedure is similar to before. Get the weekday from your data, compute the sum of daily minutes of watching, and generate a bar plot. The seaborn package is an amazing tool that generates beautiful plots with really limited code! 4.5 Binge watching In this section, the goal should be to create a plot of your top 10 binge tv shows. Before you begin, you need to decide what binge watching means to you. We decided its at least 1 hour of a TV show per day. But you should adjust that to your watching behavior. Again, you could use the familiar {dplyr} verbs for this part. You first want to filter for TV Shows and group by date and title. You then need to count episodes and minutes per day and group again by title. Now, calculate the sum of episodes and minutes per day. Here you might want to convert minutes into hours. Finally, arrange everything descending. For the plot, you can use ggplots geom_col() or whatever you think is best to present your binge series To do it our way, you could create a dataframe that only consists of the TV Shows you watched, count the number of episodes, your daily minutes, and group that by title. You can calculate the sum of episodes and minutes (or hours) per day and arrange that in descending order. You can create a bar plot or any other plot that conveys the message of your top ten bingeworthy shows! 4.6 Scatterplot with marginal density You have computed and visualized your favorite binge tv shows, but how has your watching behavior developed on Netflix since you first used it? This question is something we want to analyze now. There are different possibilities to explore this question. We will start by visualizing it via a scatterplot with marginal density. Include all the profile names for this task and make a visual comparison. What can the plot tell us about your watching behavior? To achieve this visualization calls from the packages ggExtra can be added to your ggplot. Have a look at these sources and find your code. The shown visulaization was plotted with Python, Rs output can therefore vary ! Again, seaborn is your best friend! The jointplot function will give you a great plot if you use it correctly. 4.7 Word cloud with your favorite genre Until now, even though we merged the two data sets, analyzing the Netflix movies was possible by only using each one. Lets change that! Generate a word cloud with your personal most watched genres! Is it what you expected it to be? Install the package wordcloud and start preparing your joined dataset. We are only interested in the listed_in column now. Use the mutate function to do a structure split, so each genre will get its row instead of being separated by a comma. Add unnest to your call. Extract the column and fix possible issues (e.g., white space). An easy way is to use the table() call on the listed_in and change the table directly into a data frame. For the word cloud, set a seed if you want to get the same cloud every time you run the code. The call wordcloud only needs to be filled with the respective word and frequency column. Feel free to spice it up! Use the Wordcloud package that can be installed by using: from wordcloud import WordCloud. Stopwords are words, that will not be shown on the wordcloud. 4.8 Surprise Us! This section only scratches the surface of what is possible with your Netflix data and some columns havent even been analyzed. Have you discovered anything cool and/or want to experiment with a new visualization technique to deliver a message? Here is the right place to put down anything you did and would like to do with the project that did not quite fit in the other exercise sections. If you are looking for some visualization inspiration, check out this page. Congratulations! Based on your work with fundamental data transformations and many visualizations, you now have a solid understanding of the Netflix data sets and your personal data! With this, you have completed the EDA part of the project! Dont forget to send your project results to our project submission email address (projekt@tech-academy.io) before the deadline (05.02.2023, 23:59). Thanks for being a part of TechAcademy! If you are in the advanced track your coding journey goes on with the next section! "],["content-based-recommendation-system-1.html", "5 Content-Based Recommendation System 5.1 Theory 5.2 Bag of words 5.3 Cosine Similarity 5.4 Recommendations", " 5 Content-Based Recommendation System Though Netflix offers many movies, it has a great recommendation system customized for every viewer (otherwise, we would probably be overwhelmed). So lets get to the bottom of this system and build your personalized system! There are two different recommendation systems: collaborative and content-based, but we focused on the content-based recommendation system because of its simplicity. What does that mean? It means that you will build a recommendation system based on your previous viewing history and recommend movies/shows with similar content. So your content-based recommendation will base on the description, genre, and actors column. Before you start coding you should first check out the theory behind a content-based recommendation system. 5.1 Theory One of your questions probably is How can I calculate the similarity between two movies regarding the description/genre/actors? You will calculate the similarity of movies/shows with the cosine similarity function, but we will get to that later. The hard work will be the preparation to use the cosine similarity function. We will walk you through the theory by looking at the description column. The approach for the genre and cast columns are very similar, so dont worry! To calculate the similarity between two movies/shows regarding the description, you need to first prep the column. Therefore, you need to collect all words used in the entire description column, generate new columns with each word as titles, and fill in the cells for each movie/show with 1s if the word appears or 0s if it doesnt appear. In the below chart, you can see what we mean :) The matrix on the right hand side is called a Document Term Matrix. Now let us explain what all these words above and underneath the arrow mean. Tokenize/vectorize words means each word has its column. Transfer words to lowercase is helpful since the same words should not be in two different columns just because one word has a capital letter and the other a lowercase. The next step is to remove stop words and punctuation since that is irrelevant to the similarity matrix. At last, you should also consider the stemming of each word to reduce verbs like waiting, waits, waited, etc. to wait. Maybe you should also only consider words that appear more often than once (50-100 times) to reduce the size of your data frame. FYI: this method is called bag of words :) If you are in the R track then please read the following section. Python coders can skip this part. Before we can calculate the cosine similarity matrix, you first need to transpose the matrix. What does that mean? You have to switch the rows to columns and the other way around. We have visualized the idea below. (If you dont transpose the matrix, you will calculate the similarity between each word instead of each movie.) Now lets check out that scary word cosine similarity. In our case, it calculates the similarity between two items (movies/shows) based on the description column. Mathematical speaking, it measures the cosine of the angle between two vectors projected in a multi-dimensional space. If you are further interested in the mathematical idea/function, you can check out this website. Though, this is not necessary for you to build the recommendation engine since R and Python provide packages for that function ;) The matrix below shows how the output should look like. If two movies/shows are identical, they have the value 1, and with no similarities, the value 0. So, in this case, movie 1 and 3 have the highest similarity (the identical movies have, of course, the value 1, but they arent interesting since we want a recommendation system with new movies). In contrast, movie 1 and 2 have the lowest similarity. Now you have to repeat the steps above for genre and actors so that you have 3 cosine similarity matrices. At this point, you completed the data preparation, and you can continue with the actual recommendation of some movies. The goal is to print 10 recommendations from the 5 previous movies you have watched. For example, movie 3 was the last movie you watched. Now you need to build a data frame/matrix where all movies are in the rows, and the column from movie 3 of the similarity matrices of description/genre/actors are in each column. After that, you need to generate a final column where you weigh each similarity component for the final similarity value of each movie. For example, if you say that description, genre and actors are all equally important to you, then you can choose for each a 1/3 share: You can see in the below graph what we mean :) In this case, movie 1 has a higher similarity than movie 2 regarding movie 3, so your recommendation would be to watch movie 1. Thats the theory behind the content-based recommendation system (yeah!). Phew, that was a lot! If you have any questions, please ask your mentor; they are happy to help you! If not, lets get ready to rumble! 5.2 Bag of words As mentioned in the theory chapter, you need to extract all words from the description/genre/cast column with the bag of words approach. We suggest starting with the description, continuing with the genres, and ending with the actors. Below you can see the example for the description column. 5.2.1 Creating a corpus Before you start, you first need to create a corpus to vectorize the words in each column. We recommend using the tm and SnowballC packages. Check out this website. This article gets interesting/relevant, starting with the title Create the bag of words. Since there is not a package that reliably builds a term document matrix, which is applicable to a recommendation system, you have to build it on your own. Sounds really challenging and it is kinda true. It is comparable to the steps that are done by the function in R (look tipp above). The most important functions used are here: Where you have to apply those is in your hands. For the data preparations we would recommend using lambda functions. There are some necessary changes to the dataframes to work with it in later steps. :) 5.2.2 Document Term Matrix Since you have created a corpus, you can build a document term matrix (which is on the right side of the above chart). For that, you should consider transferring the words to lowercase, removing stop words and punctuation, and regard the stemming of each word to reduce verbs like waiting, waits, waited, etc. to wait. Continue reading the article at Document term matrix for words. It explains how to create a document term matrix by editing the words mentioned above. (You can ignore the part about the weighting function TfIdf since we dont consider that for simplicity reasons.) Again at this point this function might be helpful: You can use this function on all columns that are relevant for you. After that you can fit a matrix on this column. You still have to rename some stuff after that, but we leave that to you. 5.3 Cosine Similarity Lets move forward to the cosine similarity matrix! Remember, you need 3 matrices for description, genre, and actors. Below you can see what your output should look like. 5.3.1 Preparation for R only Before you can start using the cosine similarity function, you first need to transform the document term matrix to a matrix to transpose it (so that the movie titles are in the column and the words represented in the rows.) If you have completed that, you can continue with transposing the matrix. After transposing the matrix, you can add the title names in the columns if you havent done that before. You can transpose the matrix with the basic R package, so you dont need to install anything further. Transposition of the matrix is not needed in Python. You can simply use the cosine similarity in the sklearn library. But you should be aware of the index, columns and data type after using it. 5.3.2 Function Now you should be able to use the function for the description, genre and actors column :) For the cosine similarity function, you can use the lsa package. Dont worry if the function takes a few minutes to calculate; it is a large matrix, so its only normal that the performance isnt the best. After calculating the cosine similarity function, you might want to transform it into a data frame to continue working with tidyverse. In Python you have to do all the steps for every column you use for the recommendation separately. For some columns you have to use the functions in a slightly different way. 5.4 Recommendations You are almost there, so dont give up! Before you start coding your final part for the content-based recommendation, have a look at how the output could look like. So in our example, Tobis last watched movie/show was Breaking Bad, and as recommendations, we got: There are some very interesting recommendations, but most of them make sense, so it worked! Now its your turn! One of the challenges is that not every public data set might include your last-watched movie/show. That is why we worked with a simple for loop, which iterates over your last seen movies/shows and first checks if the public data set includes that movie/show, and if so, you can build your data frame as mentioned in theory: So you take out of each similarity matrix (description/genre/actors) the column of your last seen movie and create a final similarity column. As already mentioned, you can weigh the aspects (description/genre/actors) differently and check what happens to your recommendations. For example, if you say that description, genre and actors are all equally important to you, then you can choose for each a 1/3 share: For instance: I have looked at recommendations for Inception, and one of them was Lord of the Rings, but once I changed my priorities/weighting, that recommendation changed. If the public data set doesnt include your last-watched movie/show, your loop should move on to the next movie/show you have watched. Since you only want to print recommendations for 5 movies/shows, you also need to define that your loop stops after movie/show number 5. Dont forget to sort your data frame based on the final similarity column since you want recommendations with the highest similarity. Voilà! You got yourself a self-built content-based recommendation system! Congratulations! Youve made it to the end of your TechAcademy Data Science project. After visualizing the data in the first part, youve also set up your personal content-based recommendation system in this section. If you stuck around until this part and managed to code the better part of the exercises, you have definitely earned your certificate! We hope you had fun learning Data Science with this combination of data sets and enjoyed it  at least the parts where you did not get stuck forever because of some unexplainable coding error. Do not forget to send your project results to our project submission email address before the deadline (05.02.2023, 23:59). Thank you for being a part of TechAcademy! "],["exercise-checklist.html", "6 Exercise Checklist 6.1 Part 1: Exploratory Data Analysis (Beginner + Advanced) 6.2 Part 2: Content-Based Recommendation System (motivated Beginner + Advanced)", " 6 Exercise Checklist This checklist should help you keep track of your exercises. Remember that you have to hand in satisfactory solutions of the exercises. If youre part of the beginner track, this refers to section 4 (EDA) only. If youre part of the advanced track, you have to hand in section 4 and 5. 6.1 Part 1: Exploratory Data Analysis (Beginner + Advanced) You completed at least 6 courses on Datacamp.  You completed section 4 and handed your project in at least 3 times.  You checked that you included everything you coded in the right order and added explanatory comments.  You have all your output in the desired format.  You compared your output with your group.  You sent your final work to projekt@tech-academy.io (Remember: Each team member has to hand in their final work individually, in any case it can be the same within the group).  6.2 Part 2: Content-Based Recommendation System (motivated Beginner + Advanced) Everything of the above stated.  In addition you also completed section 5 and handed your project in at least 3 times.  "],["whats-next-in-your-data-science-career.html", "7 Whats Next in Your Data Science Career? 7.1 Continue your journey on DataCamp 7.2 Data Science in General 7.3 R 7.4 Python", " 7 Whats Next in Your Data Science Career? 7.1 Continue your journey on DataCamp You can continue learning on DataCamp until the start of the next Semester! Check out the Skill and Career Tracks and make sure to complete them to receive DataCamps Statement of Accomplishment. 7.2 Data Science in General Version Control with Git Advice for Non-Traditional Data Scientists Learn from Great Data Scientists on Kaggle Great insights and helpful tips on towardsdatascience 7.3 R Version Control with Git RStudio has a nice interface that lets you enjoy the perks of Git without ever having to touch the command line  sounds great, does it? Learn how to set up the Git &amp; R workflow with Happy Git with R. R Graph Gallery Get inspiration to take your plotting to the next level. Includes code to reproduce the plots. Follow the R Master Himself and the R Community Hadley Wickham was and continues to be extremely influential on the development of R and its rise to one of the most popular data science languages. Hes behind many tools that we taught you in this semester, especially the tidyverse (including great packages such as ggplot2 and dplyr). Follow him on Twitter to get great R advice and keep up to speed with everything new to R. Following the many people behind R (not only Hadley) is a great way for acquiring deeper understanding of the language and its developments. Join the Campus useR Group in Frankfurt Theres a quite active R community in Frankfurt that meets once a month. Its open for students, professors, industry practitioners, journalists, and all people that love to use R. In those meetings, youll hear about others work, discuss new developments, and ask questions. Listen to R Podcasts Another great way to easily keep up with new developments in the Data Science/R community. Check out Not So Standard Deviations or the R-Podcast 7.4 Python Install Python Locally Until now youve only programmed using JupyterHub on the TechAcademy Server. A next step would be to install Python and Jupyter locally on your computer. This link contains the necessary information on how to install the software on Windows, iOS or Linux. Choosing the Right Editor Using Jupyter is especially useful for short data analyses. But sometimes you want to write longer scripts in Python. In these cases, it is often more convenient to use a code editor instead of Jupyter. This tutorial highlights the positive aspects of such an editor and how to choose the right one for you. Pro-Tip: Also check out the other tutorials on Real Python and check out the Community Version of PyCharm which is the most common Python IDE (Integrated Development Environment. Python Graph Gallery Get inspiration to take your plotting to the next level. Includes code to reproduce the plots. More Advanced Python Concepts You know the basic data structures in Python like lists and dictionaries. What are the next steps to improve your knowledge? This website gives good explanations for slightly more advanced concepts which can be very useful from time to time. A Deeper Understanding If you want to get a deeper understanding of the Python programming language and into typical algorithms which are used in the field of Data Science, this free book can be a good starting point. Writing Beautiful Python Code My code doesnt look nice, but it works! This might work for yourself, but often you will work on code with other people. But even if youre just coding for yourself its a good idea to follow the PEP8 style guide. Its a useful convention on how to structure and code in Python. Youll find useful resources for PEP8 here and here. Listen to Python Podcasts When you dont have time for books you can listen to Talk Python or the Python Podcast. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
